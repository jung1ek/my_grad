{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5bec643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn import LinearLayer,Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8fac2",
   "metadata": {},
   "source": [
    "Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63da6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QKV weights and shape\n",
    "seq_len = 3\n",
    "embed_dim = 4 # feature vector\n",
    "\n",
    "q_w = np.random.randn(embed_dim,embed_dim)\n",
    "k_w = np.random.randn(embed_dim,embed_dim)\n",
    "v_w = np.random.randn(embed_dim,embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2014a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention\n",
    "# input x, sequence of words\n",
    "x = np.random.randn(seq_len,embed_dim) # seq,feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdbfa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project input to linear layer for query , key and value\n",
    "Q = np.dot(x,q_w)\n",
    "K = np.dot(x,k_w)\n",
    "V = np.dot(x,v_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate attention score and output context vector\n",
    "atten_scores = np.dot(Q,K.transpose()) # Q.Kt\n",
    "# scale\n",
    "atten_scores /=np.sqrt(K.shape[-1])\n",
    "# softmaxing over each row.\n",
    "atten_weights = np.exp(atten_scores)/np.sum(np.exp(atten_scores),keepdims=True,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ac045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally context vector. applying atten weights to each seq value\n",
    "context_vec = np.dot(atten_weights,V) # seq,feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14296213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.47286368],\n",
       "       [2.95989443],\n",
       "       [2.09954924]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(atten_scores),keepdims=True,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f09050",
   "metadata": {},
   "source": [
    "Masked Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1978753b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Masked Attention\n",
    "# Creating mask\n",
    "mask = np.tril(np.ones((seq_len,seq_len)))\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83dc3752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06670566 0.         0.        ]\n",
      " [0.40852935 0.03270521 0.        ]\n",
      " [0.28575114 0.18995286 0.524296  ]]\n"
     ]
    }
   ],
   "source": [
    "# apply to attention weights\n",
    "masked_atten = atten_weights*mask\n",
    "print(masked_atten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ac68d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        ]\n",
      " [6.19848035]\n",
      " [5.23856068]]\n",
      "[[1.         0.         0.        ]\n",
      " [0.9880419  0.0119581  0.        ]\n",
      " [0.81773636 0.08217966 0.10008398]]\n"
     ]
    }
   ],
   "source": [
    "# normalize the softmax making ever row sum to 1.\n",
    "sums = np.sum(masked_atten,keepdims=True, axis=1)\n",
    "masked_atten_norm = masked_atten/sums\n",
    "print(sums)\n",
    "print(masked_atten_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14403d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.80208863 -5.54754719  0.83465224]\n",
      " [ 0.18996209 -2.3350674   0.5031281 ]\n",
      " [-0.51091133 -0.91925666  0.09602381]]\n"
     ]
    }
   ],
   "source": [
    "# Efficient Way is to used -inf instead 0, in the scores directly , then applying softmax, exp(-inf) is 0\n",
    "print(atten_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "566a7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.80208863        -inf        -inf]\n",
      " [ 0.18996209 -2.3350674         -inf]\n",
      " [-0.51091133 -0.91925666  0.09602381]]\n"
     ]
    }
   ],
   "source": [
    "# masking the attention scores\n",
    "mask = np.triu(np.ones((seq_len,seq_len)),k=1)\n",
    "masked_scores = np.where(mask,-np.inf,atten_scores)\n",
    "print(masked_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84caa8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.        ]\n",
      " [0.92587796 0.07412204 0.        ]\n",
      " [0.28575114 0.18995286 0.524296  ]]\n"
     ]
    }
   ],
   "source": [
    "# applying the softmax\n",
    "masked_atten_scores = np.exp(masked_scores)/np.sum(np.exp(masked_scores),axis=-1,keepdims=True)\n",
    "print(masked_atten_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8473e956",
   "metadata": {},
   "source": [
    "Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a4f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "\n",
    "    def forward(self,Q,K,V):\n",
    "        # calculate attention score and output context vector\n",
    "        atten_scores = np.dot(Q,K.transpose()) # Q.Kt\n",
    "        # scale\n",
    "        atten_scores /=np.sqrt(K.shape[-1])\n",
    "        # softmaxing over each row.\n",
    "        atten_weights = np.exp(atten_scores)/np.sum(np.exp(atten_scores),keepdims=True,axis=-1)\n",
    "        # finally context vector. applying atten weights to each seq value\n",
    "        context_vec = np.dot(atten_weights,V) # seq,feature vector\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f76f76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "\n",
    "    def __init__(self,embed_dim,heads):\n",
    "        self.q_w = np.random.randn(embed_dim,embed_dim)\n",
    "        self.k_w = np.random.randn(embed_dim,embed_dim)\n",
    "        self.v_w = np.random.randn(embed_dim,embed_dim)\n",
    "        self.heads = [Attention() for _ in range(heads)]\n",
    "        self.n_heads = heads\n",
    "        self.dk = embed_dim//heads\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # project input to linear layer for query , key and value\n",
    "        Q = np.dot(x,self.q_w)\n",
    "        K = np.dot(x,self.k_w)\n",
    "        V = np.dot(x,self.v_w)\n",
    "\n",
    "        # split to heads; heads,seq_len,head_embed\n",
    "        q_heads = Q.reshape(Q.shape[0],self.n_heads,self.dk).transpose(1,0,2)\n",
    "        k_heads = K.reshape(K.shape[0],self.n_heads,self.dk).transpose(1,0,2)\n",
    "        v_heads = V.reshape(V.shape[0],self.n_heads,self.dk).transpose(1,0,2)\n",
    "\n",
    "        heads = []\n",
    "        for i,head in enumerate(self.heads):\n",
    "            heads.append(head.forward(q_heads[i],k_heads[i],v_heads[i]))\n",
    "        return np.concatenate(heads,axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d385cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02859584 -1.70349588 -0.06870893 -0.86886527]\n",
      " [-0.88822215 -0.50872112 -0.45928556 -0.36587025]\n",
      " [ 0.82561654  0.87119922  0.92009989 -0.03337084]]\n"
     ]
    }
   ],
   "source": [
    "# x into heads\n",
    "# 2 heads\n",
    "heads = 2\n",
    "embed_dim = 4\n",
    "head_dim = 4//2\n",
    "x = np.random.randn(3,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e0d749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.02859584, -1.70349588],\n",
       "        [-0.06870893, -0.86886527]],\n",
       "\n",
       "       [[-0.88822215, -0.50872112],\n",
       "        [-0.45928556, -0.36587025]],\n",
       "\n",
       "       [[ 0.82561654,  0.87119922],\n",
       "        [ 0.92009989, -0.03337084]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,heads,head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38b4f36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.02859584, -1.70349588],\n",
       "        [-0.88822215, -0.50872112],\n",
       "        [ 0.82561654,  0.87119922]],\n",
       "\n",
       "       [[-0.06870893, -0.86886527],\n",
       "        [-0.45928556, -0.36587025],\n",
       "        [ 0.92009989, -0.03337084]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,heads,head_dim).transpose(1,0,2) # head,seq,head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba95f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_heads = x.reshape(3,heads,head_dim).transpose(1,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c6b8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = []\n",
    "for head in x_heads:\n",
    "    heads.append(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "862a6a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.02859584, -1.70349588],\n",
       "        [-0.88822215, -0.50872112],\n",
       "        [ 0.82561654,  0.87119922]]),\n",
       " array([[-0.06870893, -0.86886527],\n",
       "        [-0.45928556, -0.36587025],\n",
       "        [ 0.92009989, -0.03337084]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c69fe60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02859584, -1.70349588, -0.06870893, -0.86886527],\n",
       "       [-0.88822215, -0.50872112, -0.45928556, -0.36587025],\n",
       "       [ 0.82561654,  0.87119922,  0.92009989, -0.03337084]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(heads,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe5dc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "mha= MultiHeadAttention(4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a6bfdab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.40463364,  2.40482304,  1.26445166, -0.56788762],\n",
       "       [-0.2439183 , -0.26813713,  1.28104496, -0.49546929],\n",
       "       [-2.27833796, -1.09096959, -1.3063785 , -0.48213754]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96a201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nn import LinearLayer,Linear\n",
    "from tensor import Tensor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0b2c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b88832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA with Mask, for autoregressive model\n",
    "class MultiHeadAttention:\n",
    "\n",
    "    def __init__(self,embed_dim,num_heads,context_len):\n",
    "        assert embed_dim%num_heads==0,\"feature vector must be divisible by heads\"\n",
    "        self.embed_dim =  embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim//num_heads\n",
    "        self.W_query = None\n",
    "        self.W_key = None\n",
    "        self.W_value = None\n",
    "        self.out_proj = None\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3dd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(786)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
